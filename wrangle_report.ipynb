{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reporting: wragle_report\n",
    "* Create a **300-600 word written report** called \"wrangle_report.pdf\" or \"wrangle_report.html\" that briefly describes your wrangling efforts. This is to be framed as an internal document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "This project aims to wrangle, analyze, visualize and gain insight into WeRateDogs tweet data. WeRateDogs is a Twitter account that rates people's dogs with a humorous comment about the dog.\n",
    "The Dataset\n",
    "This project uses 3 datasets:\n",
    "Twitter archive enhanced: This is a csv dataset which was provided by Udacity. It holds WeRateDogs Twitter archive and provides  most of the data needed for this project.\n",
    "Image prediction dataset:  This tsv dataset results from neural network classifying the images in twitter archive data. \n",
    "Supplementary Twitter API json dataset:  This dataset is to be obtained from Twitter API using Tweepy library. Due to difficulty in obtaining approval for Twitter API, the copy that was provided by Udacity was used for this study.\n",
    "\n",
    "The Data Assessment\n",
    "The datasets was visually assessed by loading them into microsoft excel and visual studo code as text file. The visual assesment was conducted to quikly develop a mental image of the structure of the files and identify quality and tidyness issues that can be uncovered by visual assessemnt. Next the dataframe was assessed programatically by loading it into pandas data frame and executing functions to establish the shape of the dataset, column names, length, data types, row count and indentify quality and tidyness issues. Some pandas functions used during assessment: `info`, `describe`, `isnull`, `sum`, `value_counts`, `head`, `sample`, `duplicated` e.t.c.\n",
    "\n",
    "The following issues where identified:\n",
    "Quality Issues\n",
    "Tweet Archive Data\n",
    "There are rows of retweet_status_id and reply_status_id that contain non NAN values, those are obvious duplications, since they are either a retweet or reply to an original dog rating tweet.\n",
    "The project specifications specified that rating denominators are always 10, some rating_denominators values are not 10.\n",
    "in_reply_to_status_id, in_reply_to_user_id, retweeted_status_id, retweeted_status_user_id, retweeted_status_timestamp contains multiple NAN values, this columns are not needed for this analysis (there values are redundant).\n",
    "The timestamp column is an object type, which is an incorrect type for the values that it stores.\n",
    "The dogs name are not consistently capitalized. All the non capitalized dog names are either a single letter a, an, none or doesn't make sense for a dog's name.\n",
    "Values for the source column contains redundant data not needed for this analysis. e.g, it contains html anchor element, and a URL pointing to the resource or where to download resource.\n",
    "There are observations that are not actual dog rating tweets. E.g Today, 10/10, should be National Dog Rates Day\n",
    "Tweet API Data\n",
    "The contributors, coordinates, and geo columns only contain null values.\n",
    "There are 78 observations where the column in_reply_to_user_id values are set. These are all reply to tweets and not original dog rating. They are all duplications.\n",
    "The columns in_reply_to_user_id, in_reply_to_status_id are all float64 types. They should be int64 types.\n",
    "Image Prediction Data\n",
    "The columns p1, p2 and p3 contains values that doens't describe dog breeds. e.g web_site, laptop, notebook, wombat, street_sign e.t.c\n",
    "Inconsistent capitalization of predicted dog breed.\n",
    "Tidiness Issues\n",
    "Merge the three (3) dataframes into single dataframe since they all contain information about a sigle observation unit.\n",
    "The columns doggo,floofer,pupper and puppo all relates to the same variable. A Tidy version of this dataset is one in which they are all in a single column.\n",
    "The data cleaning\n",
    "I had to make copies of all the three dataframes before cleaning the data. The data cleaning process followed three primary steps: Define, code and test. I had to follow loical order while cleaning the data bacuse certain step fixes other data issues; for instance the image reply and retweet id columns on the ehanced data type had incorrect datatypes (float64 instead of int64). Dropping these columns took care of that too. Though a large part of the data cleaning process is programmatic, some process required visual examination of data. For instance visually examining the tweet texts to determine the correct rating numerator. Though I am aware that I may not have cleaned all every issues in the dataset, I am confident that have cleaned the enough to carry on analysis on the dataset.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
